{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/full_data.csv')\n",
    "col_list = data.columns.tolist()\n",
    "features = col_list.copy()\n",
    "features.remove('y')\n",
    "num_cols = [col for col in features if data[col].dtype=='int64']\n",
    "data_le = pd.read_csv('Data/encoded_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mm = data_le.copy()\n",
    "for num_col in num_cols:\n",
    "    scaler = MinMaxScaler()\n",
    "    data_mm[num_col] = scaler.fit_transform(data_le[num_col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVRfc(df, n_splits=5,  rando_state=2021, features=features, if_print=True, \n",
    "                  *args, **kwargs):\n",
    "    # Prepare KStratifiedKFOLD\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=rando_state)\n",
    "    \n",
    "    # Make copy of data\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Prepare empty lists\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    preds = []\n",
    "    \n",
    "    # Prepare int to count fold s\n",
    "    fold_number = 1\n",
    "       \n",
    "    for train, test in kf.split(data.index.values, data['y']):\n",
    "        # Prepare KNN model \n",
    "        model = RandomForestClassifier(*args, **kwargs)\n",
    "        model.fit(data.loc[train, features], data.loc[train, 'y'])\n",
    "        \n",
    "        # Make predictions\n",
    "        train_preds = model.predict(data.loc[train, features])\n",
    "        test_preds = model.predict(data.loc[test, features])\n",
    "        preds.append(test_preds)\n",
    "        \n",
    "        # Prepare ROC_AUC score\n",
    "        train_roc = metrics.roc_auc_score(data.loc[train, 'y'], train_preds)\n",
    "        test_roc = metrics.roc_auc_score(data.loc[test, 'y'], test_preds)\n",
    "        \n",
    "        # Add ROC_AUC to lis\n",
    "        train_results.append(train_roc)\n",
    "        test_results.append(test_roc)\n",
    "        \n",
    "        if if_print:\n",
    "            print(f'FOLD NUMBER: {fold_number}')\n",
    "            print(f'ROC_AUC ON TRAIN SCORE {train_roc}')\n",
    "            print(f'ROC_AUC ON TEST SCORE {test_roc}')\n",
    "                  \n",
    "        fold_number += 1 \n",
    "        \n",
    "    return train_results, test_results, preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD NUMBER: 1\n",
      "ROC_AUC ON TRAIN SCORE 0.999896054072912\n",
      "ROC_AUC ON TEST SCORE 0.7764705320312717\n",
      "FOLD NUMBER: 2\n",
      "ROC_AUC ON TRAIN SCORE 0.9997692519530665\n",
      "ROC_AUC ON TEST SCORE 0.7750502513054518\n",
      "FOLD NUMBER: 3\n",
      "ROC_AUC ON TRAIN SCORE 0.9997860962566845\n",
      "ROC_AUC ON TEST SCORE 0.7777020561782865\n",
      "FOLD NUMBER: 4\n",
      "ROC_AUC ON TRAIN SCORE 0.999822750768748\n",
      "ROC_AUC ON TEST SCORE 0.7757575946029098\n",
      "FOLD NUMBER: 5\n",
      "ROC_AUC ON TRAIN SCORE 0.9998930481283422\n",
      "ROC_AUC ON TEST SCORE 0.7809612232280239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.999896054072912,\n",
       "  0.9997692519530665,\n",
       "  0.9997860962566845,\n",
       "  0.999822750768748,\n",
       "  0.9998930481283422],\n",
       " [0.7764705320312717,\n",
       "  0.7750502513054518,\n",
       "  0.7777020561782865,\n",
       "  0.7757575946029098,\n",
       "  0.7809612232280239],\n",
       " [array([0, 0, 1, ..., 0, 0, 1]),\n",
       "  array([0, 1, 0, ..., 0, 0, 1]),\n",
       "  array([0, 0, 0, ..., 0, 0, 0]),\n",
       "  array([0, 1, 0, ..., 0, 1, 0]),\n",
       "  array([0, 0, 0, ..., 1, 0, 0])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVRfc(df=data_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rfc_h_tuning(data):\n",
    "    test_list = []\n",
    "    for i in range(60):\n",
    "        param_dict = {\n",
    "            'n_estimators': random.randrange(50, 200, 10),\n",
    "            'max_depth': random.randrange(5, 25, 2),\n",
    "            'max_features': random.randrange(2, 15, 1),\n",
    "            'min_samples_split': random.randrange(2, 10, 1)\n",
    "        }\n",
    "        param_dict['min_samples_leaf'] = random.randrange(1, param_dict['min_samples_split'])\n",
    "\n",
    "        train_results, test_results, preds = CVRfc(df=data, if_print=False, **param_dict)\n",
    "        if i % 5 == 0:\n",
    "            print(param_dict.items(), np.mean(test_results))\n",
    "        test_list.append([param_dict.items(), np.mean(test_results)])\n",
    "        \n",
    "    return test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('n_estimators', 110), ('max_depth', 21), ('max_features', 11), ('min_samples_split', 2), ('min_samples_leaf', 1)]) 0.7815912089195638\n",
      "dict_items([('n_estimators', 130), ('max_depth', 19), ('max_features', 10), ('min_samples_split', 6), ('min_samples_leaf', 5)]) 0.7789097770125462\n",
      "dict_items([('n_estimators', 190), ('max_depth', 23), ('max_features', 12), ('min_samples_split', 6), ('min_samples_leaf', 5)]) 0.778940522023204\n",
      "dict_items([('n_estimators', 80), ('max_depth', 9), ('max_features', 9), ('min_samples_split', 5), ('min_samples_leaf', 4)]) 0.7544939706254447\n",
      "dict_items([('n_estimators', 130), ('max_depth', 21), ('max_features', 7), ('min_samples_split', 7), ('min_samples_leaf', 3)]) 0.7793107068445777\n",
      "dict_items([('n_estimators', 140), ('max_depth', 5), ('max_features', 12), ('min_samples_split', 3), ('min_samples_leaf', 2)]) 0.7446385074265078\n",
      "dict_items([('n_estimators', 50), ('max_depth', 7), ('max_features', 13), ('min_samples_split', 6), ('min_samples_leaf', 3)]) 0.7493622597893184\n",
      "dict_items([('n_estimators', 80), ('max_depth', 19), ('max_features', 4), ('min_samples_split', 4), ('min_samples_leaf', 3)]) 0.7794719985655145\n",
      "dict_items([('n_estimators', 90), ('max_depth', 19), ('max_features', 5), ('min_samples_split', 4), ('min_samples_leaf', 2)]) 0.7803546851993091\n",
      "dict_items([('n_estimators', 80), ('max_depth', 11), ('max_features', 14), ('min_samples_split', 6), ('min_samples_leaf', 3)]) 0.7748363067701622\n",
      "dict_items([('n_estimators', 120), ('max_depth', 23), ('max_features', 2), ('min_samples_split', 7), ('min_samples_leaf', 5)]) 0.7704615361193511\n",
      "dict_items([('n_estimators', 50), ('max_depth', 9), ('max_features', 12), ('min_samples_split', 7), ('min_samples_leaf', 1)]) 0.7579329943978415\n"
     ]
    }
   ],
   "source": [
    "tuned_list = Rfc_h_tuning(data_le)\n",
    "sorted_list = sorted(tuned_list, key=lambda x: x[-1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[dict_items([('n_estimators', 110), ('max_depth', 21), ('max_features', 11), ('min_samples_split', 2), ('min_samples_leaf', 1)]),\n",
       "  0.7815912089195638],\n",
       " [dict_items([('n_estimators', 160), ('max_depth', 17), ('max_features', 7), ('min_samples_split', 2), ('min_samples_leaf', 1)]),\n",
       "  0.7810283581778764],\n",
       " [dict_items([('n_estimators', 100), ('max_depth', 17), ('max_features', 13), ('min_samples_split', 7), ('min_samples_leaf', 1)]),\n",
       "  0.7806364435994351],\n",
       " [dict_items([('n_estimators', 90), ('max_depth', 19), ('max_features', 5), ('min_samples_split', 4), ('min_samples_leaf', 2)]),\n",
       "  0.7803546851993091],\n",
       " [dict_items([('n_estimators', 170), ('max_depth', 23), ('max_features', 6), ('min_samples_split', 6), ('min_samples_leaf', 5)]),\n",
       "  0.7803131685043786],\n",
       " [dict_items([('n_estimators', 140), ('max_depth', 19), ('max_features', 6), ('min_samples_split', 8), ('min_samples_leaf', 4)]),\n",
       "  0.7798835279905207],\n",
       " [dict_items([('n_estimators', 50), ('max_depth', 15), ('max_features', 10), ('min_samples_split', 3), ('min_samples_leaf', 2)]),\n",
       "  0.7798320082921847],\n",
       " [dict_items([('n_estimators', 70), ('max_depth', 17), ('max_features', 6), ('min_samples_split', 3), ('min_samples_leaf', 2)]),\n",
       "  0.7795117846903757],\n",
       " [dict_items([('n_estimators', 80), ('max_depth', 19), ('max_features', 4), ('min_samples_split', 4), ('min_samples_leaf', 3)]),\n",
       "  0.7794719985655145],\n",
       " [dict_items([('n_estimators', 130), ('max_depth', 21), ('max_features', 7), ('min_samples_split', 7), ('min_samples_leaf', 3)]),\n",
       "  0.7793107068445777],\n",
       " [dict_items([('n_estimators', 190), ('max_depth', 23), ('max_features', 12), ('min_samples_split', 6), ('min_samples_leaf', 5)]),\n",
       "  0.778940522023204],\n",
       " [dict_items([('n_estimators', 130), ('max_depth', 19), ('max_features', 10), ('min_samples_split', 6), ('min_samples_leaf', 5)]),\n",
       "  0.7789097770125462],\n",
       " [dict_items([('n_estimators', 180), ('max_depth', 23), ('max_features', 11), ('min_samples_split', 7), ('min_samples_leaf', 4)]),\n",
       "  0.7787328109892301],\n",
       " [dict_items([('n_estimators', 60), ('max_depth', 13), ('max_features', 11), ('min_samples_split', 5), ('min_samples_leaf', 1)]),\n",
       "  0.7784034879981732],\n",
       " [dict_items([('n_estimators', 90), ('max_depth', 15), ('max_features', 12), ('min_samples_split', 8), ('min_samples_leaf', 4)]),\n",
       "  0.7783093976519211],\n",
       " [dict_items([('n_estimators', 180), ('max_depth', 13), ('max_features', 11), ('min_samples_split', 9), ('min_samples_leaf', 2)]),\n",
       "  0.777672532512669],\n",
       " [dict_items([('n_estimators', 80), ('max_depth', 21), ('max_features', 6), ('min_samples_split', 9), ('min_samples_leaf', 8)]),\n",
       "  0.7775976985331619],\n",
       " [dict_items([('n_estimators', 70), ('max_depth', 13), ('max_features', 11), ('min_samples_split', 2), ('min_samples_leaf', 1)]),\n",
       "  0.7775794229117008],\n",
       " [dict_items([('n_estimators', 120), ('max_depth', 13), ('max_features', 11), ('min_samples_split', 7), ('min_samples_leaf', 2)]),\n",
       "  0.777455220563492],\n",
       " [dict_items([('n_estimators', 80), ('max_depth', 19), ('max_features', 7), ('min_samples_split', 9), ('min_samples_leaf', 7)]),\n",
       "  0.7773725641400955],\n",
       " [dict_items([('n_estimators', 100), ('max_depth', 15), ('max_features', 7), ('min_samples_split', 6), ('min_samples_leaf', 1)]),\n",
       "  0.777081113499858],\n",
       " [dict_items([('n_estimators', 120), ('max_depth', 13), ('max_features', 10), ('min_samples_split', 9), ('min_samples_leaf', 3)]),\n",
       "  0.7768380508524757],\n",
       " [dict_items([('n_estimators', 50), ('max_depth', 13), ('max_features', 10), ('min_samples_split', 6), ('min_samples_leaf', 4)]),\n",
       "  0.7760577928320023],\n",
       " [dict_items([('n_estimators', 150), ('max_depth', 17), ('max_features', 3), ('min_samples_split', 5), ('min_samples_leaf', 4)]),\n",
       "  0.7752196998775618],\n",
       " [dict_items([('n_estimators', 170), ('max_depth', 11), ('max_features', 10), ('min_samples_split', 5), ('min_samples_leaf', 1)]),\n",
       "  0.7748749731083058],\n",
       " [dict_items([('n_estimators', 60), ('max_depth', 13), ('max_features', 6), ('min_samples_split', 8), ('min_samples_leaf', 1)]),\n",
       "  0.7748429365890697],\n",
       " [dict_items([('n_estimators', 80), ('max_depth', 11), ('max_features', 14), ('min_samples_split', 6), ('min_samples_leaf', 3)]),\n",
       "  0.7748363067701622],\n",
       " [dict_items([('n_estimators', 80), ('max_depth', 19), ('max_features', 2), ('min_samples_split', 6), ('min_samples_leaf', 1)]),\n",
       "  0.7747703014710041],\n",
       " [dict_items([('n_estimators', 180), ('max_depth', 23), ('max_features', 2), ('min_samples_split', 9), ('min_samples_leaf', 4)]),\n",
       "  0.7729574037703222],\n",
       " [dict_items([('n_estimators', 150), ('max_depth', 11), ('max_features', 9), ('min_samples_split', 6), ('min_samples_leaf', 4)]),\n",
       "  0.7721091651357785],\n",
       " [dict_items([('n_estimators', 120), ('max_depth', 23), ('max_features', 2), ('min_samples_split', 7), ('min_samples_leaf', 5)]),\n",
       "  0.7704615361193511],\n",
       " [dict_items([('n_estimators', 130), ('max_depth', 13), ('max_features', 3), ('min_samples_split', 8), ('min_samples_leaf', 5)]),\n",
       "  0.7647235370680335],\n",
       " [dict_items([('n_estimators', 100), ('max_depth', 9), ('max_features', 13), ('min_samples_split', 5), ('min_samples_leaf', 4)]),\n",
       "  0.760666410740581],\n",
       " [dict_items([('n_estimators', 100), ('max_depth', 9), ('max_features', 14), ('min_samples_split', 5), ('min_samples_leaf', 1)]),\n",
       "  0.7602893804818955],\n",
       " [dict_items([('n_estimators', 140), ('max_depth', 9), ('max_features', 12), ('min_samples_split', 2), ('min_samples_leaf', 1)]),\n",
       "  0.7585738438743806],\n",
       " [dict_items([('n_estimators', 50), ('max_depth', 9), ('max_features', 12), ('min_samples_split', 7), ('min_samples_leaf', 1)]),\n",
       "  0.7579329943978415],\n",
       " [dict_items([('n_estimators', 90), ('max_depth', 9), ('max_features', 11), ('min_samples_split', 7), ('min_samples_leaf', 3)]),\n",
       "  0.7561024537149622],\n",
       " [dict_items([('n_estimators', 150), ('max_depth', 9), ('max_features', 10), ('min_samples_split', 4), ('min_samples_leaf', 3)]),\n",
       "  0.7550833553094238],\n",
       " [dict_items([('n_estimators', 160), ('max_depth', 9), ('max_features', 10), ('min_samples_split', 9), ('min_samples_leaf', 1)]),\n",
       "  0.7546468727275231],\n",
       " [dict_items([('n_estimators', 50), ('max_depth', 9), ('max_features', 9), ('min_samples_split', 4), ('min_samples_leaf', 3)]),\n",
       "  0.7546262699464238],\n",
       " [dict_items([('n_estimators', 80), ('max_depth', 9), ('max_features', 9), ('min_samples_split', 5), ('min_samples_leaf', 4)]),\n",
       "  0.7544939706254447],\n",
       " [dict_items([('n_estimators', 170), ('max_depth', 9), ('max_features', 8), ('min_samples_split', 5), ('min_samples_leaf', 2)]),\n",
       "  0.7540223194493894],\n",
       " [dict_items([('n_estimators', 170), ('max_depth', 9), ('max_features', 7), ('min_samples_split', 4), ('min_samples_leaf', 1)]),\n",
       "  0.7531183166288423],\n",
       " [dict_items([('n_estimators', 190), ('max_depth', 7), ('max_features', 12), ('min_samples_split', 7), ('min_samples_leaf', 1)]),\n",
       "  0.7499381873342853],\n",
       " [dict_items([('n_estimators', 140), ('max_depth', 7), ('max_features', 12), ('min_samples_split', 3), ('min_samples_leaf', 1)]),\n",
       "  0.749561863107284],\n",
       " [dict_items([('n_estimators', 180), ('max_depth', 9), ('max_features', 4), ('min_samples_split', 2), ('min_samples_leaf', 1)]),\n",
       "  0.7494486628096904],\n",
       " [dict_items([('n_estimators', 50), ('max_depth', 7), ('max_features', 13), ('min_samples_split', 6), ('min_samples_leaf', 3)]),\n",
       "  0.7493622597893184],\n",
       " [dict_items([('n_estimators', 70), ('max_depth', 7), ('max_features', 10), ('min_samples_split', 5), ('min_samples_leaf', 1)]),\n",
       "  0.7488748428012568],\n",
       " [dict_items([('n_estimators', 120), ('max_depth', 7), ('max_features', 8), ('min_samples_split', 6), ('min_samples_leaf', 4)]),\n",
       "  0.7482160831876083],\n",
       " [dict_items([('n_estimators', 190), ('max_depth', 9), ('max_features', 4), ('min_samples_split', 7), ('min_samples_leaf', 6)]),\n",
       "  0.7481333060247032],\n",
       " [dict_items([('n_estimators', 100), ('max_depth', 5), ('max_features', 12), ('min_samples_split', 8), ('min_samples_leaf', 3)]),\n",
       "  0.7464484453282877],\n",
       " [dict_items([('n_estimators', 170), ('max_depth', 7), ('max_features', 5), ('min_samples_split', 2), ('min_samples_leaf', 1)]),\n",
       "  0.7460329823636273],\n",
       " [dict_items([('n_estimators', 180), ('max_depth', 5), ('max_features', 11), ('min_samples_split', 4), ('min_samples_leaf', 3)]),\n",
       "  0.74533772996682],\n",
       " [dict_items([('n_estimators', 110), ('max_depth', 5), ('max_features', 11), ('min_samples_split', 9), ('min_samples_leaf', 4)]),\n",
       "  0.7451668447046493],\n",
       " [dict_items([('n_estimators', 60), ('max_depth', 7), ('max_features', 4), ('min_samples_split', 4), ('min_samples_leaf', 2)]),\n",
       "  0.7448947857338849],\n",
       " [dict_items([('n_estimators', 140), ('max_depth', 5), ('max_features', 12), ('min_samples_split', 3), ('min_samples_leaf', 2)]),\n",
       "  0.7446385074265078],\n",
       " [dict_items([('n_estimators', 50), ('max_depth', 5), ('max_features', 8), ('min_samples_split', 7), ('min_samples_leaf', 1)]),\n",
       "  0.7432692379498136],\n",
       " [dict_items([('n_estimators', 140), ('max_depth', 5), ('max_features', 7), ('min_samples_split', 3), ('min_samples_leaf', 2)]),\n",
       "  0.7428894414699994],\n",
       " [dict_items([('n_estimators', 190), ('max_depth', 5), ('max_features', 4), ('min_samples_split', 2), ('min_samples_leaf', 1)]),\n",
       "  0.7357654922184298],\n",
       " [dict_items([('n_estimators', 50), ('max_depth', 7), ('max_features', 2), ('min_samples_split', 2), ('min_samples_leaf', 1)]),\n",
       "  0.7319661004242612]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
